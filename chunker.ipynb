{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunker: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [00:02<00:00, 459.66it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = LSTMTagger(os.path.join('data', 'train.txt.gz'), os.path.join('data', 'chunker'), '.tar')\n",
    "decoder_output = chunker.decode('data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "processed 23663 tokens with 11896 phrases; found: 11672 phrases; correct: 8568.\n",
    "accuracy:  84.35%; (non-O)\n",
    "accuracy:  85.65%; precision:  73.41%; recall:  72.02%; FB1:  72.71\n",
    "             ADJP: precision:  36.49%; recall:  11.95%; FB1:  18.00  74\n",
    "             ADVP: precision:  71.36%; recall:  39.45%; FB1:  50.81  220\n",
    "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
    "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
    "               NP: precision:  70.33%; recall:  76.80%; FB1:  73.42  6811\n",
    "               PP: precision:  92.40%; recall:  87.14%; FB1:  89.69  2302\n",
    "              PRT: precision:  65.00%; recall:  57.78%; FB1:  61.18  40\n",
    "             SBAR: precision:  84.62%; recall:  41.77%; FB1:  55.93  117\n",
    "               VP: precision:  63.66%; recall:  58.25%; FB1:  60.83  2108\n",
    "(73.40644276901988, 72.02420981842637, 72.70875763747455)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "The biggest change we made was through the semi-character RNN. As instructed for the baseline solution, we implemented this model to deal with noisy inputs. character_level_representation() is the baseline solution which simply creates 3 100 dimensional vectors. The first vector encodes the first character, the last vector encodes the last character, and the 3rd vector stores the character counts of all the other characters in between. Our second experimental implementation called character_level_representation_v2() was an extension of that work. In this function, we are extending that idea to encode the second, and second-to-last characters in their own vectors.\n",
    "\n",
    "Both of these functions also implement an idea that was in the \"Combating Adversarial Misspellings with Robust Word Recognition\" paper. In this paper, the authors suggest various backoff methods such as passing through the word, backing off to a neutral word, or backing off to a neutral model. We decided to implement the backoff to a neutral word model, and we chose the backoff word as \"a\". We hope that this will make the model more robust to the misspellings in the test set. We also normalize the internal character counts.\n",
    "\n",
    "Note that we needed to implement some other small changes in the codebase to have these functions work. This meant that in the training function we created an encoded tensor and passed this into the forward function. In the forward function, this was concatenated to the embedding vector. Although single line changes, we are noting these here for your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_level_representation(sentence):\n",
    "    \"\"\"\n",
    "    Encoding first and last characters and internal counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    first_characters = torch.zeros((len(sentence), len(string.printable)))\n",
    "    last_characters = torch.zeros((len(sentence), len(string.printable)))\n",
    "    other_characters = torch.zeros((len(sentence), len(string.printable)))\n",
    "\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word == '[UNK]':\n",
    "            word = 'a'\n",
    "        \n",
    "        # First and last characters\n",
    "        first_characters[i][string.printable.find(word[0])] = 1\n",
    "        last_characters[i][string.printable.find(word[-1])] = 1\n",
    "\n",
    "        # Non Edge Characters\n",
    "        for j in range(1, len(word)-1):\n",
    "            other_characters[i][string.printable.find(word[j])] += 1\n",
    "\n",
    "            # Normalize Internal Character Vector\n",
    "            tmax = torch.max(other_characters[i])\n",
    "            tmin = torch.min(other_characters[i])\n",
    "            other_characters[i] = (other_characters[i]-tmin) / (tmax - tmin)\n",
    "            \n",
    "    return torch.cat([first_characters, other_characters, last_characters], dim=1)\n",
    "\n",
    "def character_level_representation_v2(sentence):\n",
    "    \"\"\"\n",
    "    Encoding first, second, second last and last characters, and internal counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    first_characters = torch.zeros((len(sentence), len(string.printable)))\n",
    "    second_characters = torch.zeros((len(sentence), len(string.printable)))\n",
    "    last_characters = torch.zeros((len(sentence), len(string.printable)))\n",
    "    second_last_characters = torch.zeros((len(sentence), len(string.printable)))\n",
    "    other_characters = torch.zeros((len(sentence), len(string.printable)))\n",
    "\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word == '[UNK]':\n",
    "            word = 'a'\n",
    "        \n",
    "        # First and last characters\n",
    "        first_characters[i][string.printable.find(word[0])] = 1\n",
    "        last_characters[i][string.printable.find(word[-1])] = 1\n",
    "\n",
    "        if len(word) > 3:\n",
    "            second_characters[i][string.printable.find(word[1])] = 1\n",
    "            second_last_characters[i][string.printable.find(word[-2])] = 1\n",
    "\n",
    "        # Non Edge Characters\n",
    "        for j in range(2, len(word)-2):\n",
    "            other_characters[i][string.printable.find(word[j])] += 1\n",
    "\n",
    "            # Normalize Internal Character Vector\n",
    "            tmax = torch.max(other_characters[i])\n",
    "            tmin = torch.min(other_characters[i])\n",
    "            other_characters[i] = (other_characters[i]-tmin) / (tmax - tmin)\n",
    "            \n",
    "    return torch.cat([first_characters, second_characters, other_characters, second_last_characters, last_characters], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The first iteration of the model was the default code. This resulted in the following scores. One of the notable places where this model underperformed was on the `ADJP` tags. \n",
    "\n",
    "```\n",
    "processed 23663 tokens with 11896 phrases; found: 11672 phrases; correct: 8568.\n",
    "accuracy:  84.35%; (non-O)\n",
    "accuracy:  85.65%; precision:  73.41%; recall:  72.02%; FB1:  72.71\n",
    "             ADJP: precision:  36.49%; recall:  11.95%; FB1:  18.00  74\n",
    "             ADVP: precision:  71.36%; recall:  39.45%; FB1:  50.81  220\n",
    "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
    "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
    "               NP: precision:  70.33%; recall:  76.80%; FB1:  73.42  6811\n",
    "               PP: precision:  92.40%; recall:  87.14%; FB1:  89.69  2302\n",
    "              PRT: precision:  65.00%; recall:  57.78%; FB1:  61.18  40\n",
    "             SBAR: precision:  84.62%; recall:  41.77%; FB1:  55.93  117\n",
    "               VP: precision:  63.66%; recall:  58.25%; FB1:  60.83  2108\n",
    "(73.40644276901988, 72.02420981842637, 72.70875763747455)\n",
    "```\n",
    "\n",
    "The next iteration of our model was the baseline solution that implemented a semi-character RNN to deal with noisy inputs. This was denoted in the character_level_representation() function above. One important thing to note is that we initially did not not normalize the internal character count, but normalizing this resulting in a small gain on the FB1 score. We can see that our correct count went from 8568 to 9270. We can also see that the FB1 score increased by almost 5 points. This iteration of the model actually ended up being our highest-scoring solution, and we have selected this as our final submission. One interesting point is that this model scored worse on the PP and SBAR tags than the default model.\n",
    "\n",
    "```\n",
    "processed 23663 tokens with 11896 phrases; found: 12141 phrases; correct: 9270.\n",
    "accuracy:  86.68%; (non-O)\n",
    "accuracy:  87.87%; precision:  76.35%; recall:  77.93%; FB1:  77.13\n",
    "             ADJP: precision:  46.00%; recall:  20.35%; FB1:  28.22  100\n",
    "             ADVP: precision:  66.03%; recall:  43.47%; FB1:  52.42  262\n",
    "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
    "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
    "               NP: precision:  75.81%; recall:  81.64%; FB1:  78.62  6717\n",
    "               PP: precision:  91.30%; recall:  87.26%; FB1:  89.23  2333\n",
    "              PRT: precision:  61.36%; recall:  60.00%; FB1:  60.67  44\n",
    "             SBAR: precision:  82.44%; recall:  45.57%; FB1:  58.70  131\n",
    "               VP: precision:  66.33%; recall:  73.52%; FB1:  69.74  2554\n",
    "(76.35285396590066, 77.92535305985206, 77.13108957024588)\n",
    "```\n",
    "\n",
    "The final and best model iteration was a last-minute improvement. After speaking with the professor in class, we found out that we are able to tune the biderctional parameter. We expected this to outperform our previous iterations, and it did not disappoint. The FB1 score improved by over 3 points. We hypthesize that this is because the model is able to use information from two directions rather than just one, and as a result is more accurate.\n",
    "\n",
    "```\n",
    "processed 23663 tokens with 11896 phrases; found: 12036 phrases; correct: 9609.\n",
    "accuracy:  88.63%; (non-O)\n",
    "accuracy:  89.42%; precision:  79.84%; recall:  80.78%; FB1:  80.30\n",
    "             ADJP: precision:  49.00%; recall:  21.68%; FB1:  30.06\n",
    "             ADVP: precision:  69.23%; recall:  47.49%; FB1:  56.33\n",
    "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
    "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
    "               NP: precision:  79.48%; recall:  83.21%; FB1:  81.30\n",
    "               PP: precision:  93.81%; recall:  90.58%; FB1:  92.16\n",
    "              PRT: precision:  71.43%; recall:  55.56%; FB1:  62.50\n",
    "             SBAR: precision:  83.33%; recall:  56.96%; FB1:  67.67\n",
    "               VP: precision:  70.18%; recall:  78.56%; FB1:  74.13\n",
    "(79.83549351944168, 80.77505043712172, 80.30252381748286)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
