{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 Report\n",
    "\n",
    "Group Members: Greg Mehdiyev, Brendan Artley\n",
    "\n",
    "In this assignment, we were tasked with arranging tokens into meaningful groups. This is more commonly known as phrasal chunking. This task was not only about accurately executing the task on known vocabulary, but having a robust model that would perform well on unseen words. Our group decided to implement a back-off to a neutral word strategy where unknown words are encoded as the word 'a'. We also experimented with extensions of the baseline model which we explain more in detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "The biggest change we made was through the semi-character RNN. As instructed for the baseline solution, we implemented this model to deal with noisy inputs. character_level_representation() is the baseline solution which simply creates 3 100 dimensional vectors. The first vector encodes the first character, the last vector encodes the last character, and the 3rd vector stores the character counts of all the other characters in between. Our second experimental implementation called character_level_representation_v2() was an extension of that work. In this function, we are extending that idea to encode the second, and second-to-last characters in their own vectors.\n",
    "\n",
    "Both of these functions also implement an idea that was in the \"Combating Adversarial Misspellings with Robust Word Recognition\" paper. In this paper, the authors suggest various backoff methods such as passing through the word, backing off to a neutral word, or backing off to a neutral model. We decided to implement the backoff to a neutral word model, and we chose the backoff word as \"a\". We hope that this will make the model more robust to the misspellings in the test set. Note that we also normalize the internal character count vector. \n",
    "\n",
    "Note that we needed to implement some other small changes in the codebase to have these functions work. This meant that in the training function we created an encoded tensor and passed this into the forward function. In the forward function, this was concatenated to the embedding vector. Although single line changes, we are noting these here for your reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The first iteration of the model was the default code. This resulted in the following scores. One of the notable places where this model underperformed was on the `ADJP` tags. \n",
    "\n",
    "```\n",
    "processed 23663 tokens with 11896 phrases; found: 11672 phrases; correct: 8568.\n",
    "accuracy:  84.35%; (non-O)\n",
    "accuracy:  85.65%; precision:  73.41%; recall:  72.02%; FB1:  72.71\n",
    "             ADJP: precision:  36.49%; recall:  11.95%; FB1:  18.00\n",
    "             ADVP: precision:  71.36%; recall:  39.45%; FB1:  50.81\n",
    "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
    "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
    "               NP: precision:  70.33%; recall:  76.80%; FB1:  73.42\n",
    "               PP: precision:  92.40%; recall:  87.14%; FB1:  89.69\n",
    "              PRT: precision:  65.00%; recall:  57.78%; FB1:  61.18\n",
    "             SBAR: precision:  84.62%; recall:  41.77%; FB1:  55.93\n",
    "               VP: precision:  63.66%; recall:  58.25%; FB1:  60.83\n",
    "(73.40644276901988, 72.02420981842637, 72.70875763747455)\n",
    "```\n",
    "\n",
    "The next iteration of our model was a semi-character RNN to deal with noisy inputs. This was denoted in the character_level_representation() function above. One important thing to note is that we initially did not not normalize the internal character count, but normalizing this resulted in a small gain on the FB1 score. We can see that our correct count went from 8568 to 9270. We can also see that the FB1 score increased by almost 5 points. This iteration of the ended up being our 2nd highest-scoring solution.\n",
    "\n",
    "```\n",
    "processed 23663 tokens with 11896 phrases; found: 12141 phrases; correct: 9270.\n",
    "accuracy:  86.68%; (non-O)\n",
    "accuracy:  87.87%; precision:  76.35%; recall:  77.93%; FB1:  77.13\n",
    "             ADJP: precision:  46.00%; recall:  20.35%; FB1:  28.22\n",
    "             ADVP: precision:  66.03%; recall:  43.47%; FB1:  52.42\n",
    "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
    "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
    "               NP: precision:  75.81%; recall:  81.64%; FB1:  78.62\n",
    "               PP: precision:  91.30%; recall:  87.26%; FB1:  89.23\n",
    "              PRT: precision:  61.36%; recall:  60.00%; FB1:  60.67\n",
    "             SBAR: precision:  82.44%; recall:  45.57%; FB1:  58.70\n",
    "               VP: precision:  66.33%; recall:  73.52%; FB1:  69.74\n",
    "(76.35285396590066, 77.92535305985206, 77.13108957024588)\n",
    "```\n",
    "\n",
    "The final and best model iteration was a last-minute improvement. After speaking with the professor in class, we found out that we are able to tune the biderctional parameter. We expected this to outperform our previous iterations, and it did not disappoint. The FB1 score improved by over 3 points. We hypthesize that this is because the model is able to use information from two directions rather than just one, and as a result is more accurate.\n",
    "\n",
    "```\n",
    "processed 23663 tokens with 11896 phrases; found: 12036 phrases; correct: 9609.\n",
    "accuracy:  88.63%; (non-O)\n",
    "accuracy:  89.42%; precision:  79.84%; recall:  80.78%; FB1:  80.30\n",
    "             ADJP: precision:  49.00%; recall:  21.68%; FB1:  30.06\n",
    "             ADVP: precision:  69.23%; recall:  47.49%; FB1:  56.33\n",
    "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
    "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
    "               NP: precision:  79.48%; recall:  83.21%; FB1:  81.30\n",
    "               PP: precision:  93.81%; recall:  90.58%; FB1:  92.16\n",
    "              PRT: precision:  71.43%; recall:  55.56%; FB1:  62.50\n",
    "             SBAR: precision:  83.33%; recall:  56.96%; FB1:  67.67\n",
    "               VP: precision:  70.18%; recall:  78.56%; FB1:  74.13\n",
    "(79.83549351944168, 80.77505043712172, 80.30252381748286)\n",
    "```\n",
    "\n",
    "## Contributions\n",
    "\n",
    "Greg and Brendan: Baseline, Researching Papers, Experimenting with enhancements, report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
